{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongkornsettasompop/anaconda3/envs/mi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import math\n",
    "import copy\n",
    "import wandb\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np # for transformation\n",
    "\n",
    "import torch # PyTorch package\n",
    "import torchvision # load datasets\n",
    "import torchvision.transforms as transforms # transform data\n",
    "from torch.autograd import Variable\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn # basic building block for neural neteorks\n",
    "import torch.nn.functional as F # import convolution functions like Relu\n",
    "import torch.optim as optim # optimzer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkImage(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    pass\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "non_augmentned = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "gray = transforms.Compose([\n",
    "    transforms.Grayscale()\n",
    "    ])\n",
    "\n",
    "augmentations = {\n",
    "    'rotation': transforms.Compose([transforms.RandomRotation(degrees=(0,90))]),\n",
    "    'rotation2': transforms.Compose([transforms.RandomRotation(degrees=(270,360))]),\n",
    "    'norm': transforms.Compose([transforms.Normalize(mean=[0.42, 0.42, 0.42],std=[0.42, 0.42, 0.42] )]),\n",
    "    'affine': transforms.Compose([ transforms.RandomAffine(degrees=(0, 90), scale=(0.5, 0.9))]),\n",
    "    'affine2': transforms.Compose([ transforms.RandomAffine(degrees=(270, 360), scale=(0.5, 0.9))]),\n",
    "    'blur': transforms.Compose([transforms.GaussianBlur(kernel_size=(3, 7), sigma=(0.1, 3))]),\n",
    "    'sharpness': transforms.Compose([transforms.RandomAdjustSharpness(40)]),\n",
    "    'contrast': transforms.Compose([transforms.ColorJitter(contrast=(1,2))]),\n",
    "}\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "  ''' function to show image '''\n",
    "  #img = img / 2 + 0.5 # unnormalize\n",
    "  npimg = img.numpy() # convert to numpy objects\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './dataset/'\n",
    "print(os.listdir(base_dir))\n",
    "for base in os.listdir(base_dir):\n",
    "    if base == \"train\":\n",
    "        train_datasets = []\n",
    "        for augmentation_name, augmentation_transform in augmentations.items():\n",
    "            combined_transform = transforms.Compose([img_transforms,augmentation_transform])\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                root=os.path.join(base_dir, base),\n",
    "                transform=combined_transform,  # Combine main and augmentation transforms\n",
    "                is_valid_file=checkImage\n",
    "            )\n",
    "            train_datasets.append(dataset)\n",
    "\n",
    "        new_train = torch.utils.data.ConcatDataset(train_datasets)\n",
    "        \n",
    "        \n",
    "    elif base == \"val\":\n",
    "        val_datasets = []\n",
    "        for augmentation_name, augmentation_transform in augmentations.items():\n",
    "            combined_transform = transforms.Compose([img_transforms,augmentation_transform])\n",
    "            print(combined_transform)\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                root=os.path.join(base_dir, base),\n",
    "                transform=combined_transform,  # Combine main and augmentation transforms\n",
    "                is_valid_file=checkImage\n",
    "            )\n",
    "            val_datasets.append(dataset)\n",
    "\n",
    "        new_val = torch.utils.data.ConcatDataset(val_datasets)\n",
    "    elif base == \"test\":\n",
    "        test_data = torchvision.datasets.ImageFolder(root = os.path.join(base_dir,base), transform = non_augmentned, is_valid_file = checkImage)\n",
    "        new_test = test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, PyTorch’s data loaders are set to a batch_size of 1.\n",
    "BATCH_SIZE = 1000\n",
    "train_data_loader = torch.utils.data.DataLoader(new_train, batch_size = BATCH_SIZE,shuffle=False, num_workers=2)\n",
    "val_data_loader  = torch.utils.data.DataLoader(new_val, batch_size = BATCH_SIZE,shuffle=True, num_workers=2) \n",
    "test_data_loader  = torch.utils.data.DataLoader(new_test, batch_size = 2000,shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "num_step =math.ceil(len(train_data_loader.dataset) / BATCH_SIZE)\n",
    "\n",
    "print(type(train_data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ('scgc','scgp','wedo')\n",
    "\n",
    "# sample = next(iter(train_data_loader))\n",
    "# imgs, lbls = sample\n",
    "# print(lbls)\n",
    "# print(imgs.shape)\n",
    "\n",
    "# # call function on our images # +29\n",
    "# origi_img_pos = 16\n",
    "# step = 59\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos ],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos +(step)],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos +(step*2)],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos +(step*3)],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos +(step*4)],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos +(step*5)],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos +(step*6)],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs[origi_img_pos +(step*7)],nrow=15,padding=20))\n",
    "# imshow(torchvision.utils.make_grid(imgs,nrow=15,padding=20))\n",
    "# print(' '.join('%s' % classes[lbls[j]] for j in range(len(imgs))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from torchsummary import summary\n",
    "net = ResNet(ResidualBlock, [3, 4, 6, 3],num_classes=3)\n",
    "summary(net, (3, 224, 224))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #wand setup\n",
    "#\"weightname\":\"S12_3-9_fir8-14\"\n",
    "#name=f\"CNN_S12_3-9_fir8-14\",\n",
    "wandb.login()\n",
    "wand = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"AR-classification\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"res50_AR_12\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      # 0.0000001\n",
    "      config={\n",
    "      \"learning_rate\": 0.0001,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"Organization\",\n",
    "      \"epochs\": 200,\n",
    "      \"weightname\":\"res50_12\",\n",
    "      \"num_step_per_epoch\" : num_step\n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wand.config\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=config.learning_rate, weight_decay = 0.001, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    loader_train = train_data_loader,\n",
    "    loader_test =test_data_loader,\n",
    "    vail_loader = val_data_loader,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    device = 'cuda',\n",
    "    wand = wand\n",
    ")\n",
    "\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"./save_weight/res50_12/0.3732_res50_12_0.3732_82.7586.pth\"\n",
    "# # reload\n",
    "net = ResNet(ResidualBlock, [3, 4, 6, 3])\n",
    "net.load_state_dict(torch.load(PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('scgc','scgp','wedo')\n",
    "sample = next(iter(test_data_loader))\n",
    "imgs, lbls = sample\n",
    "print(type(imgs))\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(imgs))\n",
    "print('GroundTruth: ', ' '.join('%s' % classes[lbls[j]] for j in  range(len(imgs))))\n",
    "\n",
    "outputs = net(imgs)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%s' % classes[predicted[j]]\n",
    "                              for j in range(len(imgs))))\n",
    "\n",
    "wrong_img = []\n",
    "for i in range(len(imgs)):\n",
    "    if classes[lbls[i]] != classes[predicted[i]]:\n",
    "        wrong_img.append(imgs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wrong_img))\n",
    "imshow(torchvision.utils.make_grid(wrong_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_data_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(labels)\n",
    "        print(predicted)\n",
    "        \n",
    "        print(total)\n",
    "        print(correct)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(labels, predicted)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['scgc','scgp','wedo'])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
